[
["index.html", "A guide to eyeTrackR Chapter 1 EyeTrackR 1.1 What can EyeTrackR do? 1.2 Some history", " A guide to eyeTrackR Hayward J. Godwin 2019-10-11 Chapter 1 EyeTrackR EyeTrackR consists of a set of R functions, bundled into a package, that are geared towards analysing eye-tracking datasets. For now, it can handle output from SR Research’s Eyelink eye trackers and the software that comes with them. 1.1 What can EyeTrackR do? EyeTrackR has a wide array of functions available but the core goals for it are speed and accuracy. For speed, it uses the popular data.table R package. For accuracy…well, in some senses that is up to you. Many things can go wrong with complex datasets that are hundreds of thousands of rows in length (indeed, the errors may be sufficiently subtle that it’s hard to detect them). The solution to dealing with that, which is adopted by eyeTrackR, is to implement a number of ways of checking, double-checking and triple-checking your data to make sure that as little as possible slips through the cracks. Many of these checks have been inspired by errors I’ve caught in my own code, but have also been inspired by requests from editors and reviewers when writing papers. 1.2 Some history I started working on what would become this R package back in 2011 and 2012. Around that time, I was a post-doc, surviving on scraps from different research projects. In order to survive, and because I enjoy coding, I ended up helping out various other people with their R code and eye-tracking datasets. I kept using the same basic scripts repeatedly, as you do. Then, the scripts became functions, and eventually the functions became eyeTrackR. The first version was released around that time - the first commit on github for the package was April 2nd, 2012. I used it extensively since that time, and, because I wasn’t sure what it would involve, I never really got around to formalising it and uploading it to CRAN. In more recent times, I’ve been developing my working practices, and am moving more towards the open science approach that is becoming more and more important these days. With that in mind, I wanted to get eyeTrackR finished and in a state that others could use. It seems to me that consistency of data processing, filtering and cleaning methods - all of which are offered by eyeTrackR - could be beneficial to a wider audience. I’ve seen so many times, and in so many papers, people making mistakes where they’ve missed things that eyeTrackR can check for them very easily. I’m hoping that it can help to avoid those mistakes in the future. Indeed, I’m sure I’ve made plenty myself… "],
["installation.html", "Chapter 2 Installation 2.1 Installing via Devtools 2.2 Removing eyeTrackR", " Chapter 2 Installation 2.1 Installing via Devtools From R using the devtools package: library(devtools) install_github(&#39;hjgodwin/eyetrackR/source&#39;) 2.2 Removing eyeTrackR If you want to remove the package either to update it or because you just want to get rid of it, run the following from R: remove.packages(pkgs=&quot;eyeTrackR&quot;) "],
["gettingstarted.html", "Chapter 3 Getting Started 3.1 The Basics 3.2 Files you’ll Need", " Chapter 3 Getting Started This guide is written in such a way that you can go, chapter by chapter, through the text. Just to say from the start, I’m going to assume from the beginning that you are familiar with the Eyelink system, and know how to output data from SR Research DataViewer. 3.1 The Basics The functions in eyeTrackR are mostly split up into two categories: Organise functions and Analyse functions.1 Organise functions are there to help you clean, check, and, well, organise your data. Analyse functions are there to work out means of common dependent variables and put them together in a format that can easily then be analysed by standard statistical methods (e.g., ANOVAs), as well as visualising those dependent variables. For most of my projects, I have two key sets of files: ORGANISE__markup.R : this marks up the dataset and cleans it, as you’ll see in this chapter. ANALYSE__XYZ.R : this either contains all of my analyses (if there aren’t that many), OR I split this file up, so that I have one ANALYSE__XYZ.R file for each dependent variable. In this chapter, and the next, I’ll walk you through the steps for putting together a set of analyses for a project. 3.2 Files you’ll Need To start with, you’ll need to output three different types of report from SR Research DataViewer. You’ll need: Fixation Report Message Report Interest Area Report When I generate these reports, I typically get DataViewer to output every single column of data it can. It has a whole bunch of different ones, but my preference is to be greedy. The simple reason for this is that DataViewer can take a long, long time to load up and output all your data, so you don’t want to have to go back into it and do everything again just because you’ve missed a vital column. That being said, if you have a lower-spec or older computer, or are short on space, you may struggle with these files. When saving these reports, it’s best to save them as tab-delimeted text files (.txt). Future versions may include OrganiZe and AnalyZe functions, who knows.↩ "],
["organise.html", "Chapter 4 Organise Functions 4.1 Overview 4.2 Organise Functions Walkthrough", " Chapter 4 Organise Functions The stages and information given in this section follows the examples that come with the EyeTrackR package. Here, we are going to walk through the why and the how of organising eye-tracking data, ready for analyses. 4.1 Overview There are a handful of key tasks that take place in the organise functions. They are as follows: Marking up events in each trial Removing outliers or other fixations that you’re not interested in I’ll now talk about each of these in detail before providing a walkthrough of the functions with example code and data. 4.1.1 Marking up Events In a typical study, there will by multiple events in a single trial. For example, you might ask participants to fixate a certain point, after which the main display is presented. After some time has passed, the participant may make a response of some kind, terminating the trial. Here’s a diagram: For this example, there are three events: Display fixation point. Display a trial. Here, participants search for a T shape amongst L shapes. Participant response. They press one button (button XYZ) if they spot a T shape, and a different button (button XYZ) if there are no T shapes present. Even if you have the most carefully-controlled study, it’s unlikely that even the first and second of these events will occur at the same time in each trial. There will be variability when they occur. This means you can’t just assume that an event occurred at the same time each trial: you actually have to check the timings for each trial on an individual basis. If you don’t do this, in a best-case scenario, you’ll be adding noise to your measures, and in a worst-case scenario, your results will be incorrect. So, the solution is to examine when each event occurred for that participant and for that trial. The functions relating to events and contingencies enable you to do this. 4.1.1.1 Contingencies Contingencies enable you to draw together the events in a trial. Imagine our simple example above. A participant may make several fixations on the fixation cross, then make several more during the main trial before making a response. Indeed, in most experiments, you would probably hope they would do this. But that will not always be the case. Some participants, on some trials, will fixate the fixation point, and hold a fixation throughout the display of the main trial, and then make a response, without making any new fixations. The first point here is that fixations of this type will naturally be longer than when a participant makes multiple fixations around a display - a fixation held throughout a trial will have a longer duration than one which is made right at the end of a trial. We can see in the example below that different participants approached the task in different ways. I WOULD PUT IN A FIGURE HERE HIGHLIGHTING THIS So, what are contingencies? Contingencies were developed to help you work out the various events that occurred during a fixation. They are there to help you make an informed decision about the fixations that you want to include in your analyses. The contingency functions add a column to your fixation report, called FIXATION_CONTINGENCY. This column tells you what events occurred during the fixation in question. If a fixation lasted through both a fixation cross and the main trial display, it would have a contingency of FIX_CROSS__MAIN_DISPLAY . If it only lasted through the fixation cross time period, it would have a contingency of FIX_CROSS. Likewise, if the fixation occurred only during the main display, it would have a contingency of MAIN_DISPLAY. This is highlighted in the figure below. ANOTHER FIGURE SHOWING THINGS The nice thing about the way that contingencies work is that they are marked up automatically. All you have to do is tell EyeTrackR the names of the events to consider when labelling the FIXATION_CONTINGENCY column. This means that any contingencies which do occur can be detected, and, furthermore, also means that you can be clear about exactly what types of events are occurring during the fixations that you are analysing for your various measures. 4.1.2 Removing Fixations You’re Not Interested In 4.2 Organise Functions Walkthrough 4.2.1 Getting Started To use events and contingencies, two files will be needed: a fixation report, and a message report. The message report serves as a ‘fool-proof’ way of examining the timings and occurrences of events.2 We begin by loading the fixation report and message report - these are the ones that come pre-packaged with EyeTrackR. It’s a dummy dataset. You can of course replace them with whatever you want. data(fixationreport) data(messagereport) Next, we begin ‘marking up’ the events, which is why I usually call the file with my organise functions in ORGANISE__Markup.R. The first step is to replace the spaces in any messages with underscores. This is needed as a safety measure because spaces in the event labels can disrupt the process of working out contingencies. # REPLACE SPACES IN MESSAGES messagereport &lt;- organise.message.replace_spaces(messagereport) 4.2.2 Descriptives Now, we cant get some descriptive statistics from the message report. This is helpful in getting an understanding of what happened in our dataset. It can also be helpful in highlighting any weird issues that have cropped up. # TAKE A LOOK organise.message.descriptives(messagereport) From that, we get the following - formatted nicely here for easy reading: Table 4.1: Message report descriptives CURRENT_MSG_TEXT EVENT_COUNT TIME_MEAN TIME_MIN TIME_MAX DISPLAY_START 15000 30.4704 26.0000 35.000 DISPLAY_CHANGE 15000 255.4810 251.0000 260.000 BUTTON_PRESS 15000 1137.6133 404.4914 1982.368 The table above, which you get in the console output by running organise.message.descriptives, has some useful information in it that you can use to spot and diagnose any problems with your data. First, it lists all of the messages it finds in your message report file, in turn, and puts them in the CURRENT_MSG_TEXT column. Then, it gives you a count of how many times each message has been detected in your message report, the mean time each message occurred from the start of each trial, and then, as well as that, the maximum and minimum times each message occurred. There will always be variability in terms of when events occur in a trial, but this does enable you to make sure that, for the most part at least, things happened when they were supposed to. One thing you can do at this point is a back-of-the-envelope calculation and check to make sure all the trials are there as you would expect. Assuming you know how many participants were in your dataset, and how many trials, you should be able to check to make sure the right number of trials have started (i.e., had a DISPLAY_START message appear). If that’s not the case, then you may need to delve deeper to find why data are missing. Now that you have checked to make sure things appear as they should - here, we have XYZ partcipants in our example dataset who took part in XYZ trials, giving us XYZ trials across everyone, which is what is being reported in the table, we can continue. 4.2.3 Message Markup Our next step is to begin to markup the fixation report file based on the messages we are interested in. As we talked about above, because fixations can span multiple messages or events, we need to get the information on when each message of interest occurred in each trial. The organise.message.markup function is the start of that process. Let’s run it first then talk about what it does to your fixation report: # MARKUP FIXATION REPORT fixationreport &lt;- organise.message.markup(message_df= messagereport, fixreport_df = fixationreport, message=&quot;DISPLAY_START&quot;) fixationreport &lt;- organise.message.markup(message_df= messagereport, fixreport_df = fixationreport, message=&quot;DISPLAY_CHANGE&quot;) If you now take a look at fixreport - which I would recommend you do - you’ll now see some new columns have been added. One is called DISPLAY_START and the other is called DISPLAY_CHANGE. If you look closely, what this has done is: Work out when each DISPLAY_START and DISPLAY_CHANGE occurred in each of the trials. Take the timings of those messages and then add that information to the fixation report. This process also gives you some diagnostic information that helps to check for any problems. It compares the number of rows your fixation report had before you marked it up with each new message with what happens to the fixation report after you have marked it up with each new message. If the number of rows are different, that suggests something has gone wrong. Usually this is the case that you have got participants or trials that are missing from either the fixation report or the message report. If you get the following message, then you have nothing to worry about because the dataset has been matched up perfectly: Difference between input and output rows: 0 4.2.3.1 Behavioural Markup We now have two of the major events in each trial marked up into our fixation report. The only thing that is missing is the other main event in each trial: the response made by each participant! Here, there are two ways to markup the fixation report. If you use a button box, you can use the method here. If you are not using a button box, you’ll need to use the method described in Behavioural Markup (Alternative). To mark up the fixation report with the timings of each response, as well as the button press of each response, and finally the outcome (correct or incorrect) of each response, we can use organise.responses.markup. This function needs to be fed the fixation report you are using, as well as the column which states what the correct response on each trial is. Here, since the button boxes you can connect to Experiemnt Builder get given numbers (1-8), the correct response lists the correct numerical value of the button that needs to be pressed on each trial. Let’s run the code: # NOW DO ACCURACY AND RT MARKUP fixationreport &lt;- organise.responses.markup(fixationreport, &quot;CORRECT_RESPONSE&quot;) This function has added the following columns to your fixation report file: BUTTON_NUMBER - the button number pressed on that trial. OUTCOME - the outcome (‘CORRECT’ or ‘INCORRECT’ of the response). RESPONSE_TIME - the time from the start of the trial that the response was made. It’s worth noting that RESPONSE_TIME isn’t necessarily what you want to use as your Reaction Time measure, since, with this expeirment, as in many others, the display doesn’t actuall appear for people to look at until DISPLAY_CHANGE. We’ll talk more about this later. 4.2.3.2 Behavioural Markup (Alternative) TBA 4.2.4 Fixation Contingencies We now know at what point in each trial each of the three major events occurred- the DISPLAY_START events, the DISPLAY_CHANGE events, and the RESPONSE_TIME events. We know what button participants pressed, and we know if they were right or wrong in each trial. The final step now is to markup the fixation contingencies. Again, these are important in helping us be sure that we’ve included what we want to include when it comes to analysing our eye-tracking data. To get going with this, we need to use the organise.message.fix_contingencies function. You need to feed this function the fixation report you’re using, as well as a list of the messages you want to mark up in terms of your contingencies. Note that you don’t need to include all the messages here, just the ones you’re interested in. Let’s run it as follows: # NOW MARK UP FIXATION CONTINGENCIES fixationreport&lt;-organise.message.fix_contingencies(fixationreport, list(&quot;DISPLAY_START&quot;, &quot;DISPLAY_CHANGE&quot;, &quot;RESPONSE_TIME&quot;)) If you now look at your fixation report, a bunch of new columns have been added to it. They are as follows: WITHIN_PRE_DISPLAY_START - this lets you know if a given fixation started before the first message you were interested in. In this case, the first message we were interested in is DISPLAY__START, so this column is helpful in letting us focus in on fixations that occurred before even the fixation cross came up. You might be wondering how there can possibly be any time before the fixation cross which literally starts the trial, but in fact, the eye-tracker actually turns on for a little while before the display comes up. Any fixation labelled as TRUE under WITHIN_PRE_DISPLAY_START began before the fixation cross came up. WITHIN_DISPLAY_START - this lets you know if a given fixation was occuring when the DISPLAY_START message came up. The cells say TRUE if that is the case and FALSE if not. WITHIN_DISPLAY_CHANGE - this lets you know if a given fixation was occuring when the DISPLAY_CHANGE message came up. The cells say TRUE if that is the case and FALSE if not. WITHIN_RESPONSE_TIME - this lets you know if a given fixation was occuring when the RESPONSE_TIME message came up. The cells say TRUE if that is the case and FALSE if not. FIXATION_CONTINGENCY - this is the most important column added by the organise.message.fix_contingencies function. It pastes together all of the message names that each fixation spanned. Let’s give some more detailed examples of how to interepret the important FIXATION_CONTINGENCY column. It’ll help if we begin by looking at the final columns of the fixation report after organise.message.fix_contingencies has been run: Table 4.2: Fixation report final columns WITHIN_PRE_DISPLAY_START WITHIN_DISPLAY_START WITHIN_DISPLAY_CHANGE WITHIN_RESPONSE_TIME FIXATION_CONTINGENCY TRUE TRUE FALSE FALSE PRE_DISPLAY_START__DISPLAY_START FALSE TRUE FALSE FALSE DISPLAY_START FALSE TRUE TRUE FALSE DISPLAY_START__DISPLAY_CHANGE FALSE FALSE TRUE FALSE DISPLAY_CHANGE FALSE FALSE TRUE FALSE DISPLAY_CHANGE FALSE FALSE TRUE FALSE DISPLAY_CHANGE FALSE FALSE TRUE FALSE DISPLAY_CHANGE FALSE FALSE TRUE FALSE DISPLAY_CHANGE FALSE FALSE TRUE TRUE DISPLAY_CHANGE__RESPONSE_TIME Browsing through these, you may have already worked out what FIXATION_CONTINGENCY really does. Let’s give some examples: The fixation in the first row began before DISPLAY_START and then ended during the period of time when the fixation cross was up. For that reason it gets a contingency of PRE_DISPLAY_START__DISPLAY_START. The fixation in the second row began when the fixation cross was up, and ended when the fixation cross was up, giving it a contingency of DISPLAY_START. The fixation in the third row began when the fixation cross was up, and ended when the main search array was visible. For that reason, it gets a contingency of DISPLAY_START__DISPLAY_CHANGE. The next five fixations began and ended when the search array was visible, so they get a contingency of DISPLAY_CHANGE. The final fixation began when the search array was visible, and ended when the response was made. For that reason, this fixation gets a contingency of DISPLAY_CHANGE____RESPONSE_TIME. With the fixation report marked up in this manner, you can then, at a later point, be careful in how you select your data down to focus only on the trials and/or fixations that you are definitely interested in. 4.2.5 Behavioural Data You already have the responses, outcomes and reaction times marked up for each trial at this point. But one problem is that these will require some work to convert to, say, mean accuracy rates. This is because the fixation report has one row per fixation, rather than one row per trial. Therefore, if you want the mean accuracy rate for your participants, you’ll need to crunch the data down to one row per trial before computing your means. Fortunately, EyeTrackR can do this for you! Our first step in doing this is to work out the TRUE_RT column. This is as follows: # SET UP TRUE RT fixationreport[,TRUE_RT:=RESPONSE_TIME-DISPLAY_START,] This takes the response time from the display start time, to enable us to focus just on the time that we are interested in. Next, you just need to run analyse.behavioural.data, feed it the fixation report, and the list of columns you want to aggregate by (using the aggregation_column_list input). Here, in the example below, the output is saved to the **behaviouralData* data.table. behaviouralData &lt;- analyse.behavioural.data(fixationreport, aggregation_column_list = list(&#39;TRIALTYPE_TEXT&#39;)) The behaviouralData table looks like this: Table 4.3: Summarised Behavioural Data RECORDING_SESSION_LABEL TRIALTYPE_TEXT MEDIAN_RT MEAN_RT CORRECT_COUNT TOTAL_TRIALS ACCURACY 1 YES 1157.400 1185.641 1423 2449 0.5810535 1 NO 1250.787 1183.069 1012 2473 0.4092196 2 YES 1250.100 1220.968 1196 2509 0.4766839 2 NO 1285.536 1252.586 1136 2579 0.4404808 3 YES 1229.856 1184.251 1281 2408 0.5319767 3 NO 1223.563 1205.574 1142 2502 0.4564349 4 YES 1216.778 1198.994 1239 2475 0.5006061 4 NO 1427.213 1304.166 1280 2563 0.4994147 5 YES 1329.680 1248.196 1254 2509 0.4998007 5 NO 1226.644 1177.300 1214 2442 0.4971335 6 YES 1268.918 1228.710 1327 2562 0.5179547 6 NO 1252.378 1209.104 1124 2462 0.4565394 7 YES 1171.000 1156.252 1304 2438 0.5348646 7 NO 1239.448 1208.071 1133 2450 0.4624490 8 YES 1271.286 1227.473 1295 2498 0.5184147 8 NO 1291.235 1256.557 1283 2580 0.4972868 9 YES 1392.445 1323.624 1310 2587 0.5063780 9 NO 1149.848 1130.382 1014 2392 0.4239130 10 YES 1323.778 1242.543 1150 2491 0.4616620 10 NO 1343.560 1259.842 1357 2568 0.5284268 11 YES 1252.289 1233.529 1189 2476 0.4802100 11 NO 1291.085 1256.649 1324 2563 0.5165821 12 YES 1286.929 1237.908 1282 2537 0.5053212 12 NO 1272.208 1230.583 1277 2557 0.4994134 13 YES 1228.433 1206.480 1225 2425 0.5051546 13 NO 1201.048 1149.940 1068 2476 0.4313409 14 YES 1214.384 1162.218 1061 2438 0.4351928 14 NO 1287.243 1235.081 1273 2624 0.4851372 15 YES 1184.089 1161.135 1063 2462 0.4317628 15 NO 1262.098 1221.638 1358 2489 0.5456006 16 YES 1281.403 1229.739 1256 2445 0.5137014 16 NO 1124.128 1132.306 1060 2475 0.4282828 17 YES 1243.181 1210.754 1298 2459 0.5278569 17 NO 1248.391 1209.368 1291 2529 0.5104784 18 YES 1395.318 1289.188 1372 2532 0.5418641 18 NO 1240.013 1205.530 1255 2476 0.5068659 19 YES 1223.599 1175.338 1223 2362 0.5177815 19 NO 1280.064 1216.492 1322 2547 0.5190420 20 YES 1205.162 1185.567 1285 2486 0.5168946 20 NO 1324.557 1274.230 1331 2608 0.5103528 21 YES 1247.392 1206.089 1220 2416 0.5049669 21 NO 1300.559 1267.571 1384 2574 0.5376845 22 YES 1280.249 1204.834 1143 2454 0.4657702 22 NO 1340.408 1260.651 1228 2507 0.4898285 23 YES 1237.279 1168.773 1282 2470 0.5190283 23 NO 1259.285 1222.358 1436 2565 0.5598441 24 YES 1298.970 1227.991 1290 2504 0.5151757 24 NO 1268.487 1240.864 1316 2474 0.5319321 25 YES 1229.703 1190.179 1075 2423 0.4436649 25 NO 1277.152 1238.156 1335 2566 0.5202650 26 YES 1277.503 1241.456 1313 2525 0.5200000 26 NO 1223.005 1217.925 1291 2511 0.5141378 27 YES 1182.850 1185.837 1079 2383 0.4527906 27 NO 1307.262 1254.601 1144 2519 0.4541485 28 YES 1229.227 1205.312 1162 2508 0.4633174 28 NO 1200.175 1168.751 1072 2513 0.4265818 29 YES 1263.412 1209.283 1208 2526 0.4782264 29 NO 1240.007 1179.010 1128 2472 0.4563107 30 YES 1226.636 1201.704 1093 2443 0.4474007 30 NO 1232.063 1201.976 1120 2528 0.4430380 Note a few useful columns in that table to help you back-calculate and double-check to make sure everything is going ok. You should look through this to be certain. Is there the right number of trials? The right number of participants? Does it look like anything is missing? Has the accuracy rate been calculated properly? Also, do the reaction times make sense? Again, the goal here is to help you be certain that there are no problems with your data and catch those problems as early as possible. 4.2.6 Final Checks We are almost ready to save our marked-up fixation report. It’s worth at this point to suggest running some checks on our data. Have a go at running the organise.checks.random_trial function. This is a simple one - the function does, in fact, do nothing at all to your code. All it does it pick, from a random trial and participant, the fixations from that trial and display them in the console for you. You can repeatedly run this function to inspect all sorts of trials to make sure that any code you have run on your data makes sense and has worked how you think it should have done. # RANDOM TRIAL TO CHECK THINGS OUT organise.checks.random_trial(fixationreport) I can’t tell you the number of times I have thought things were working as they should, but then this function has shown me the error of my ways! This is particularly worth running multiple times for eye-tracking datasets since they can often be huge (hundreds of thousands of fixations), making it virtually impossible to conduct a visual inspection of everything. 4.2.6.1 The final check of contingencies # FIX CONTINGENCIES organise.contingencies.descriptives(fixationreport) 4.2.7 Removing Trials # REMOVE MISSING EVENTS - HERE, TRIALS WHICH LACKED A RESPOSNE foo &lt;- organise.message.removals(fixreport_df=fixationreport, required_message_list=list(&quot;DISPLAY_CHANGE&quot;, &quot;RESPONSE_TIME&quot;)) organise.contingencies.descriptives(fixationreport) # THIS SHOWS WE HAVE NO UNCLASSIFIED FIXATIONS, GOOD! 4.2.8 Time to save # REMOVALS df_final_for_analysis &lt;- organise.exclusions.fix_durations(fixreport_df=fixationreport) # THAT&#39;S IT! LET&#39;S NOW SAVE THE FINAL THING ########################################################## write.table(df_final_for_analysis, &quot;df_final_for_analysis.txt&quot;, row.names=FALSE) If you rely only on the CURRENT_FIX_MSG_X columns in the fixation report to determine when events occurs, you wil make errors and fail to detect when events occur (occasionally, and it’s very difficult to determine). For that reason, eyeTrackR uses only the message report to determine event timing and occurrences. In fact, I’ve seen people use only these columns in DataViewer, or worse, guess when events occurred.↩ "]
]
